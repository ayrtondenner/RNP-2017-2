1) 
 
2) A camada de convolução é responsável por fazer a comparação entre as matrizes de entrada e as matrizes que compõem as "características" que esses dados de entrada podem conter. Ao final, a camada de convolução retorna várias matrizes, uma para cada matriz de característica, indicando a semelhança da matriz de entrada com a respectiva característica.
 
3) A camada ReLu é responsável por aplicar a função ReLu em todos os valores de uma respectiva matriz. A função ReLu tem por funcionamento igualar a zero todos os valores negativos em uma determinada matriz, de forma a ainda manter a não-linearidade em seus valores resultantes (ou seja, no resultado da função ReLu ao receber uma matriz).
 
4) Em uma rede neural artificial, ao aumentar o número de camadas da rede, caso estejamos utilizando funções de ativação como a sigmóide ou a hiperbólica, como a derivada de cada uma delas resulta em números entre 0 e 1 ou -1 e 1, a medida que o número de camadas aumenta e a retropropagação faz com que tenhamos mais e mais multiplicações entre números menores que 1, as camadas mais profundas vão recebendo números cada vez menores e cada vez mais próximos de zero. Caso atinjam zero na derivada da função de ativação anterior, as camadas não vão conseguir atualizar os seus respectivos pesos. Com a função ReLu, não temos mais um valor máximo de 1 para saturar as derivadas, porém ainda assim temos problemas ao derivar resultados negativos, pois estes sempre serão igual a zero, enquanto os resultados positivos vão em direção ao infinito.
 
5) Na camada convolucional, a ideia de compartilhamento de pesos é presente pois os pesos de uma matriz de características são compartilhados entre vários valores da matriz de entrada, ou seja, diferentes valores da matriz de entrada compartilham e entram em contato com o mesmo peso em relação a uma mesma matriz de característica.
 
6) A camada de pooling age diminuindo o tamanho da matriz de entrada, ao agrupar valores semelhantes em apenas um único valor na mesma matriz. Com isso, o tamanho da matriz de entrada vai gradualmente diminuindo, diminuindo também o custo computacional do total da rede convolucional, sem perder informações importantes do dado de entrada, pois a proximidade de valores é preservada.
 
7) Caso o deslize do filtro seja igual a 1, o resultado final será 28x28x5
 
8) Uma rede convolucional apresenta como partes principais: a camada de convolução, a camada de pooling, e uma rede "fully-connected". A camada de convolução tem por função gerar matrizes que são resultados de comparações entre os dados de entrada e as matrizes de características (ou features); a camada de pooling tem por função gerar uma diminuição do tamanho das matrizes trabalhadas, por meio de agrupamento de valores semelhantes nos valores das matrizes; e a rede "fully-connected" atua como uma rede neural multi-camadas clássica, trabalhando com as matrizes resultantes das outras operações, e conectando todos os neurônios das suas camadas para produzir um resultado (ao contrário dos valores na camada convolucional que são localmente conectados).
