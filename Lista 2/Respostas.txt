1 - Não necessariamente, pois caso uma camada "escondida" com vários neurônios converja para uma camada final com apenas um neurônio, essa última camada irá desenhar apenas um hiperplano de separação, impedindo então a divisão de problemas não-lineares.

2 - A função de ativação tem como objetivo principal adicionar a não-linearidade na rede neural. Com isso, a rede neural consegue resolver problemas que uma rede neural com funções lineares de ativação não conseguiriam resolver. Além disso, caso a rede neural fosse formada por vários neurônios com funções lineares, pela propriedade da função linear é possível substituir vários neurônios com funções de ativação linear por apenas UM neurônio com função de ativação linear, o que indica a falta de capacidade de resolução de problemas de neurônios com função de ativação linear, e também impede a separação de classes que não são linearmente separáveis. Além disso, a função de ativação também controla o intervalo de valores que um neurônio pode retornar, impedindo que o acréscimo de valores atinja um número muito alto no final da rede neural, pois o resultado dos neurônios sempre estará dentro do intervalo da função de ativação.

3 - Como cada neurônio só consegue construir apenas uma única superfície de separação no plano do problema, uma rede neural múltipla consegue construir várias superfícies de separação, uma superfície desenhada para cada neurônio da camada oculta, de maneira que no final, a rede neural de múltiplas camadas adquire a habilidade de separar determinados problemas não-lineares, dado a junção das superfícies de separação conseguirem separar as classes do problema.

4 - O maior problema nas redes neurais de múltiplas camadas é a falta de precisão em relação as informações de soma ponderada e derivadas de erros que são propagadas pela rede de maneira progressiva e regressiva, respectivamente. Ou seja, como os cálculos em sua grande maioria resultam em dízimas, esses números são arredondados durante os cálculos, o que significa que, com o aumento horizontal ou vertical da rede neural de múltiplas camadas, teremos um resultado que é um arredondamento de um arredondamento, perdendo a precisão e a eficácia do refinamento dos pesos de acordo com o erro. Também podemos colocar como dificuldade o custo computacional de uma rede neural de múltiplas camadas, que é bem maior em relação a rede neural simples.

5 - A regra da cadeia é uma afirmação que indica que se uma função A é composta pela função B, e a mesma função B é composta pela função C, então é possível partir da função A e atingir a função C, obviamente passando pela função B.

6 - A regra da cadeia é uma fórmula para calcular a derivada de uma composição de duas ou mais funções.

7 - As derivadas parciais são utilizadas na rede neural de múltiplas camadas para encontrarem o gradiente das funções de erro entre suas partes, como soma ponderada de um neurônio, função de ativação de um neurônio, erro total da rede neural, dentre outros. Ao se descobrir, por meio das derivadas parciais e da regra da cadeia o gradiente da derivada do erro total em relação a um determinado peso da rede neural, esse peso é ajustado em relação contrária ao gradiente encontrado, ou seja, em direção ao mínimo daquela função.

8 - Não necessariamente. Dado a questão da perda de precisão no arredondamento dos resultados dos cálculos que são passados como parâmetros para outras funções, na medida que se aumenta a quantidade de camadas, ocorre uma diminuição na eficácia quanto a correção dos pesos em relação ao erro total. Ou seja, caso tenhamos uma rede neural de múltiplas camadas muito grande, teremos o problema da correção dos pesos das camadas opostas ao erro total receberam uma alteração insuficiente ao que eles realmente deveriam receber.

9 - 

10 - O gradiente descendente, por utilizar toda a base de dados como parâmetro no cálculo do refinamento dos pesos, situa-se inicialmente em um ponto na função de erro, e a partir desse ponto evolui os refinamentos via derivação da função. Porém, por desenvolver a partir de um ponto em específico da função de erro, o gradiente descendente corre o risco de atingir um mínimo local, haja visto que considera apenas um ponto na função de erro do modelo. Por outro lado, o gradiente descendente estocástico tem como vantagem que, ao utilizar uma linha por vez da base dados (ou um grupo de linhas), o gradiente estocástico acaba passando por vários pontos na função de erro (ao invés de apenas um ponto), o que aumenta as chances de atingir um mínimo global dentro da função de erro, pois ele irá desenvolver o refinamento dos pesos partindo de diferentes pontos na função de erro do sistema. Obviamente que há um trade-off em relação a utilizar o gradiente descendente estocástico, que se traduz em um cálculo mais errático e também mais demorado em relação a se aproximar do alvo da função de erro.
 
11 - OK 